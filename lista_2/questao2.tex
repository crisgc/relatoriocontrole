\chapter*{Questão 2}

{\it Implemente o método dos mínimos quadrados offline e teste o programa com o
processo da questão 1, simulando variações lentas em seus parâmetros ($\pm
10\%$) e avaliando o comportamento das estimativas.}

{\it Apresente gráficos mostrando as estimativas, os valores reais dos
parâmetros, o sinal de controle, a saída real e a estimada com cada modelo
utilizado.}

\vspace{0.5cm}

\noindent{\bf Resolução:}

\vspace{0.25cm}

O sistema da questão 1, consiste na função de transferência dada pela Eq.
\ref{eq:sist}:

\begin{equation}\label{eq:sist}
H(s) = \frac{Y(s)}{U(s)} = \frac{0,5}{(s + 0,5)(s + 1)}
\end{equation}

A discretização obtida na questão 1 é novamente exibida na Eq.
\ref{eq:sist_disc},

\begin{equation}\label{eq:sist_disc}
H(z) = \frac{Y(z)}{U(z)} = \frac{0,009056 z + 0,008194}{z^2 - 1,724 z + 0,7408}
\end{equation}

Sabendo que a função de transferência de um sistema é definida como sendo a {\it
Transformada de Laplace} da resposta ao impulso com condições iniciais nulas, e
que seu equivalente em $\mathcal{Z}$ possui as mesmas características, então:

\begin{eqnarray}
\frac{Y(z)}{U(z)} & = & \frac{0,009056 z + 0,008194}
                             {z^2 - 1,724 z + 0,7408}\nonumber\\
Y(z)(z^2 - 1,724 z + 0,7408) & = & U(z)(0,009056 z + 0,008194)\nonumber\\
z^2Y(z) - 1,724zY(z) + 0,7408Y(z) & = & 0,009056zU(z) +
                                        0,008194U(z)\label{eq:sist_z}
\end{eqnarray}

Aplicando a transformada $\mathcal{Z}$ inversa na Eq. \ref{eq:sist_z}, tem-se:

\begin{eqnarray}
z^2Y(z) - 1,724zY(z) + 0,7408Y(z) & = & 0,009056zU(z) + 0,008194U(z)
\quad\stackrel{\mathcal{Z}^{-1}}{\Longrightarrow}\quad\nonumber\\
\stackrel{\mathcal{Z}^{-1}}{\Longrightarrow}\quad
y(k+2) - 1,724y(k+1) + 0,7408k(k) & = & 0,009056u(k+1) + 0,008194u(k)\label{eq:eq_a_dif_sist}
\end{eqnarray}

Generalizando o modelo obtido na Eq. \ref{eq:eq_a_dif_sist}, tem-se que:

\begin{eqnarray}
y(k+2) + a_1 y(k+1) + a_2 y(k) & = & b_1 u(k+1) + b_2 u(k)\nonumber\\
y(k) + a_1 y(k-1) + a_2 y(k-2) & = & b_1 u(k-1) + b_2u(k-2)\nonumber
\end{eqnarray}

Isolando $y(k)$, tem-se:

\begin{equation}\label{eq:eq_a_dif_geral}
y(k) = - a_1 y(k-1) - a_2 y(k-2) + b_1 u(k-1) + b_2u(k-2)
\end{equation}

O estimador de mínimos quadrados não-recursivo considera um sistema representado
por uma equação a diferenças semelhante a Eq. \ref{eq:eq_a_dif_geral}. Segundo
\citeasnoun{coelho:2004}, haverão $n_a + n_b + 1$ parâmetros a serem estimados
pelo algoritmo e para determinar os valores de $a_i$, deve-se utilizar as
medidas de entrada e saída do processo.

Assim sendo, de posse da equação generalizada dos parâmetros do sistema,
utilizar o método dos mínimos quadrados {\it offline} consiste em excitar a
planta com um determinado sinal de entrada e armazenar os valores de saída
obtidos para executar o algoritmo não-recursivo. Assim sendo, pode-se dizer que
o processo de identificação é realizado de uma só vez, ou em batelada.

Supondo que exista um sistema discreto que possa ser descrito conforme {\it
modelo de regressão linear}:

\begin{equation}\label{eq:mode_reg_lin}
\mb{Y} = \mb{X\theta} + \mb{e}
\end{equation}

\noindent tal que $\mb{Y}$ corresponde à saída do sistema, $\mb{X}$ é um vetor
determinístico conhecido, $\mb{\theta}$ é o vetor de parâmetros a serem
estimados e $\mb{e}$ corresponde ao erro do modelo. Deseja-se então, estimar o
vetor $\mb{\theta}$ a partir de $N$ experimentos, de tal forma que:

\begin{eqnarray}
\mb{Y}_1 & = & \mb{X}_1\mb{\theta}_1 + \mb{e}_1\nonumber\\
\mb{Y}_2 & = & \mb{X}_2\mb{\theta}_2 + \mb{e}_2\nonumber\\
& & \vdots\nonumber\\ 
\mb{Y}_N & = & \mb{X}_N\mb{\theta}_N + \mb{e}_N\nonumber\\
\end{eqnarray}

\noindent considerando:

\begin{equation*}
\mb{\theta} = \left[
\begin{array}{c}
a_1\\
a_2\\
\vdots\\
a_n\\
\hline
b_1\\
b_2\\
\vdots\\
b_m
\end{array}
\right] \qquad
\mb{X} = \left[
\begin{array}{cccc}
-Y(1) & -Y(0) & u(1) & u(0)\\
-Y(2) & -Y(1) & u(2) & u(1)\\
\vdots & \vdots & \vdots & \vdots\\
-Y(N-1) & -Y(N-n) & u(N-1) & u(N-m)
\end{array}
\right]
\end{equation*}

\begin{equation*}
\mb{Y} = \left[
\begin{array}{c}
y_1\\
y_2\\
\vdots\\
y_N
\end{array}
\right] \qquad
\mb{e} = \left[
\begin{array}{c}
e_1\\
e_2\\
\vdots\\
e_N
\end{array}
\right]
\end{equation*}

O método dos mínimos quadrados tem por objetivo realizar a estimativa de
$\mb{\theta}$ de modo a minimizar a função de erro $J$, tal que:

\begin{equation}
J = \sum_{K = 1}^N e^2(K) = \mb{e}^T\mb{e}
\end{equation}

Pela Eq. \ref{eq:mode_reg_lin}:

\begin{equation}
\mb{Y} = \mb{X\theta} + \mb{e} \quad\Longrightarrow\quad \mb{e} = \mb{Y} - \mb{X\theta}
\end{equation}

Assim, a função de erro $J$, pode ser reescrita como:

\begin{equation}\label{eq:J}
J = (\mb{Y} - \mb{X\theta})^T(\mb{Y} - \mb{X\theta})
\end{equation}

\noindent que possui mínimo quando:

\begin{equation*}
\left.\frac{\partial J}{\partial \theta}\right|_{\theta = \hat{\mb{\theta}}} = 0
\end{equation*}

Considerando que:

\begin{equation*}
\frac{\partial (\mb{Ax} + \mb{b})^T\mb{C}(\mb{Dx} + \mb{e})}{\partial \mb{x}} =
(\mb{Dx} + \mb{e})^T\mb{C}^T\mb{A} + (\mb{Ax} + \mb{b})^T\mb{CD}
\end{equation*}

\noindent e fazendo:

\begin{equation*}
\begin{array}{c@{\qquad}c@{\qquad}c}
\mb{A} = -\mb{X} & \mb{x} = \mb{\theta} & \mb{b} = \mb{Y}\\
\mb{C} = \mb{I}  & \mb{D} = -\mb{X}     & \mb{e} = \mb{Y}
\end{array}
\end{equation*}

\noindent na Eq. \ref{eq:J}, tem-se que:

\begin{eqnarray}
\frac{\partial J}{\partial \mb{\theta}} & = & 
(-\mb{X\theta}+\mb{Y})^T(-\mb{X}) + (-\mb{X\theta}+\mb{Y})^T(-\mb{X})\nonumber\\
& = & -2 \left[(-\mb{X\theta}+\mb{Y})\right]^T\mb{X}\nonumber\\
& = & -2 \left[(-\mb{X\theta})^T + \mb{Y}^T\right]\mb{X}\nonumber\\
& = & -2 \left[-\mb{\theta}^T\mb{X}^T + \mb{Y}^T\right]\mb{X}\nonumber\\
& = & 2 \mb{\theta}^T\mb{X}^T\mb{X} -2 \mb{Y}^T\mb{X}\label{eq:deduc}
\end{eqnarray}

Como $\D\frac{\partial J}{\partial \mb{\theta}} = 0$ (escalar), então,
pode-se transpor a Eq. \ref{eq:deduc}, de tal forma que:

\begin{eqnarray}
\frac{\partial J}{\partial \mb{\theta}} & = & 
2\mb{X}^T(\mb{\theta}^T\mb{X}^T)^T - 2\mb{X}^T\mb{Y}\nonumber\\
0 & = & 2\mb{X}^T(\mb{X\theta}) - 2\mb{X}^T\mb{Y}\nonumber\\
0 & = & 2\mb{X}^T\mb{X\theta} - 2\mb{X}^T\mb{Y}
\end{eqnarray}

Substituindo $\mb{\theta}$ por $\hat{\mb{\theta}}$ e isolando
$\hat{\mb{\theta}}$, tem-se que:

\begin{equation}\label{eq:theta_est}
\hat{\mb{\theta}} = (\mb{X}^T\mb{X})^{-1} \mb{X}^T\mb{Y}
\end{equation}

Algumas observações podem ser feitas acerca da Eq. \ref{eq:theta_est}:

\begin{itemize}
    \item A solução existirá quando $(\mb{X}^T\mb{X})^{-1}$ for não singular
    \item A sequência escolhida das entradas $u(k)$ deverá garantir essa
          não singularidade
    \item Quando não há a presença de ruídos $\hat{\mb{\theta}}$ pode ser
          encontrado em $n+m$ passos
    \item A matriz $\mb{X}$ cresce na medida em que $N$ cresce
\end{itemize}
